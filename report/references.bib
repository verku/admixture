@article{Ewels2016,
   abstract = {Motivation: Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis. Results: We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization. Availability and implementation: MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at http://multiqc.info.},
   author = {Philip Ewels and Måns Magnusson and Sverker Lundin and Max Käller},
   doi = {10.1093/bioinformatics/btw354},
   issn = {14602059},
   issue = {19},
   journal = {Bioinformatics},
   pages = {3047-3048},
   pmid = {27312411},
   publisher = {Oxford University Press},
   title = {MultiQC: summarize analysis results for multiple tools and samples in a single report},
   volume = {32},
   year = {2016},
}
@article{Okonechnikov2015,
   author = {Okonechnikov, Konstantin and Conesa, Ana and García-Alcalde, Fernando},
   title = "{Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data}",
   journal = {Bioinformatics},
   volume = {32},
   number = {2},
   pages = {292-294},
   year = {2015},
   abstract = "{Motivation: Detection of random errors and systematic biases is a crucial step of a robust pipeline for processing high-throughput sequencing (HTS) data. Bioinformatics software tools capable of performing this task are available, either for general analysis of HTS data or targeted to a specific sequencing technology. However, most of the existing QC instruments only allow processing of one sample at a time.Results: Qualimap 2 represents a next step in the QC analysis of HTS data. Along with comprehensive single-sample analysis of alignment data, it includes new modes that allow simultaneous processing and comparison of multiple samples. As with the first version, the new features are available via both graphical and command line interface. Additionally, it includes a large number of improvements proposed by the user community.Availability and implementation: The implementation of the software along with documentation is freely available at http://www.qualimap.org.Contact:  meyer@mpiib-berlin.mpg.deSupplementary information:  Supplementary data are available at Bioinformatics online.}",
   issn = {1367-4803},
   doi = {10.1093/bioinformatics/btv566},
   url = {https://doi.org/10.1093/bioinformatics/btv566},
   eprint = {https://academic.oup.com/bioinformatics/article-pdf/32/2/292/49016552/bioinformatics\_32\_2\_292.pdf},
}
@article{Danecek2021,
   abstract = {Background: SAMtools and BCFtools are widely used programs for processing and analysing high-throughput sequencing data. They include tools for file format conversion and manipulation, sorting, querying, statistics, variant calling, and effect analysis amongst other methods. Findings: The first version appeared online 12 years ago and has been maintained and further developed ever since, with many new features and improvements added over the years. The SAMtools and BCFtools packages represent a unique collection of tools that have been used in numerous other software projects and countless genomic pipelines. Conclusion: Both SAMtools and BCFtools are freely available on GitHub under the permissive MIT licence, free for both non-commercial and commercial use. Both packages have been installed >1 million times via Bioconda. The source code and documentation are available from https://www.htslib.org.},
   author = {Petr Danecek and James K. Bonfield and Jennifer Liddle and John Marshall and Valeriu Ohan and Martin O. Pollard and Andrew Whitwham and Thomas Keane and Shane A. McCarthy and Robert M. Davies},
   doi = {10.1093/gigascience/giab008},
   issn = {2047217X},
   issue = {2},
   journal = {GigaScience},
   keywords = {bcftools,data analysis,high-throughput sequencing,next generation sequencing,samtools,variant calling},
   pmid = {33590861},
   publisher = {Oxford University Press},
   title = {Twelve years of SAMtools and BCFtools},
   volume = {10},
   year = {2021},
}
@article{Li2009SAM,
   abstract = {Summary: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAM tools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. © 2009 The Author(s).},
   author = {Heng Li and Bob Handsaker and Alec Wysoker and Tim Fennell and Jue Ruan and Nils Homer and Gabor Marth and Goncalo Abecasis and Richard Durbin},
   doi = {10.1093/bioinformatics/btp352},
   issn = {13674803},
   issue = {16},
   journal = {Bioinformatics},
   pages = {2078-2079},
   pmid = {19505943},
   title = {The Sequence Alignment/Map format and SAMtools},
   volume = {25},
   year = {2009},
}
@article{McKenna2010,
   abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS - the 1000 Genome pilot alone includes nearly five terabases - make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management infrastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide polymorphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas. © 2010 by Cold Spring Harbor Laboratory Press.},
   author = {Aaron McKenna and Matthew Hanna and Eric Banks and Andrey Sivachenko and Kristian Cibulskis and Andrew Kernytsky and Kiran Garimella and David Altshuler and Stacey Gabriel and Mark Daly and Mark A. DePristo},
   doi = {10.1101/gr.107524.110},
   issn = {10889051},
   issue = {9},
   journal = {Genome Res},
   pages = {1297-1303},
   pmid = {20644199},
   title = {The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data},
   volume = {20},
   year = {2010},
}
@article{Li2009BWA,
   abstract = {Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. Results: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ∼10-20× faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. © 2009 The Author(s).},
   author = {Heng Li and Richard Durbin},
   doi = {10.1093/bioinformatics/btp324},
   issn = {13674803},
   issue = {14},
   journal = {Bioinformatics},
   pages = {1754-1760},
   pmid = {19451168},
   title = {Fast and accurate short read alignment with Burrows-Wheeler transform},
   volume = {25},
   year = {2009},
}
@article{Chen2018,
   abstract = {Motivation Quality control and preprocessing of FASTQ files are essential to providing clean data for downstream analysis. Traditionally, a different tool is used for each operation, such as quality control, adapter trimming and quality filtering. These tools are often insufficiently fast as most are developed using high-level programming languages (e.g. Python and Java) and provide limited multi-threading support. Reading and loading data multiple times also renders preprocessing slow and I/O inefficient. Results We developed fastp as an ultra-fast FASTQ preprocessor with useful quality control and data-filtering features. It can perform quality control, adapter trimming, quality filtering, per-read quality pruning and many other operations with a single scan of the FASTQ data. This tool is developed in C++ and has multi-threading support. Based on our evaluation, fastp is 2-5 times faster than other FASTQ preprocessing tools such as Trimmomatic or Cutadapt despite performing far more operations than similar tools. Availability and implementation The open-source code and corresponding instructions are available at https://github.com/OpenGene/fastp.},
   author = {Shifu Chen and Yanqing Zhou and Yaru Chen and Jia Gu},
   doi = {10.1093/bioinformatics/bty560},
   issn = {14602059},
   issue = {17},
   journal = {Bioinformatics},
   pages = {i884-i890},
   pmid = {30423086},
   publisher = {Oxford University Press},
   title = {fastp: an ultra-fast all-in-one FASTQ preprocessor},
   volume = {34},
   year = {2018},
}
@article{Köster2012,
   abstract = {Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames. © The Author 2012. Published by Oxford University Press. All rights reserved.},
   author = {Johannes Köster and Sven Rahmann},
   doi = {10.1093/bioinformatics/bts480},
   issn = {14602059},
   issue = {19},
   journal = {Bioinformatics},
   pages = {2520-2522},
   pmid = {22908215},
   publisher = {Oxford University Press},
   title = {Snakemake—a scalable bioinformatics workflow engine},
   volume = {28},
   year = {2012},
}
@article{Smit2008,
   author = {Arian F.A. Smit and Robert Hubley},
   title = {RepeatModeler Open-1.0},
   url = {<http://www.repeatmasker.org>},
   year = {2008},
}
@article{Kutschera2022,
   abstract = {Many wild species have suffered drastic population size declines over the past centuries, which have led to ‘genomic erosion’ processes characterized by reduced genetic diversity, increased inbreeding, and accumulation of harmful mutations. Yet, genomic erosion estimates of modern-day populations often lack concordance with dwindling population sizes and conservation status of threatened species. One way to directly quantify the genomic consequences of population declines is to compare genome-wide data from pre-decline museum samples and modern samples. However, doing so requires computational data processing and analysis tools specifically adapted to comparative analyses of degraded, ancient or historical, DNA data with modern DNA data as well as personnel trained to perform such analyses. Here, we present a highly flexible, scalable, and modular pipeline to compare patterns of genomic erosion using samples from disparate time periods. The GenErode pipeline uses state-of-the-art bioinformatics tools to simultaneously process whole-genome re-sequencing data from ancient/historical and modern samples, and to produce comparable estimates of several genomic erosion indices. No programming knowledge is required to run the pipeline and all bioinformatic steps are well-documented, making the pipeline accessible to users with different backgrounds. GenErode is written in Snakemake and Python3 and uses Conda and Singularity containers to achieve reproducibility on high-performance compute clusters. The source code is freely available on GitHub ( https://github.com/NBISweden/GenErode ). GenErode is a user-friendly and reproducible pipeline that enables the standardization of genomic erosion indices from temporally sampled whole genome re-sequencing data.},
   author = {Verena E. Kutschera and Marcin Kierczak and Tom van der Valk and Johanna von Seth and Nicolas Dussex and Edana Lord and Marianne Dehasque and David W.G. Stanton and Payam Emami Khoonsari and Björn Nystedt and Love Dalén and David Díez-del-Molino},
   doi = {10.1186/S12859-022-04757-0},
   issn = {1471-2105},
   issue = {1},
   journal = {BMC Bioinformatics},
   keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
   pages = {1-17},
   publisher = {BioMed Central},
   title = {GenErode: a bioinformatics pipeline to investigate genome erosion in endangered and extinct species},
   volume = {23},
   url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04757-0},
   year = {2022},
}
@misc{Smit2013,
   author = {Arian F.A. Smit and Robert Hubley and Phil Green},
   title = {RepeatMasker Open-4.0},
   url = {<http://www.repeatmasker.org>},
   year = {2013},
}
@article{Alexander2009,
   abstract = {Population stratification has long been recognized as a confounding factor in genetic association studies. Estimated ancestries, derived from multi-locus genotype data, can be used to perform a statistical correction for population stratification. One popular technique for estimation of ancestry is the model-based approach embodied by the widely applied program structure. Another approach, implemented in the program EIGENSTRAT, relies on Principal Component Analysis rather than model-based estimation and does not directly deliver admixture fractions. EIGENSTRAT has gained in popularity in part owing to its remarkable speed in comparison to structure. We present a new algorithm and a program, ADMIXTURE, for model-based estimation of ancestry in unrelated individuals. ADMIXTURE adopts the likelihood model embedded in structure. However, ADMIXTURE runs considerably faster, solving problems in minutes that take structure hours. In many of our experiments, we have found that ADMIXTURE is almost as fast as EIGENSTRAT. The runtime improvements of ADMIXTURE rely on a fast block relaxation scheme using sequential quadratic programming for block updates, coupled with a novel quasi-Newton acceleration of convergence. Our algorithm also runs faster and with greater accuracy than the implementation of an Expectation-Maximization (EM) algorithm incorporated in the program FRAPPE. Our simulations show that ADMIXTURE's maximum likelihood estimates of the underlying admixture coefficients and ancestral allele frequencies are as accurate as structure's Bayesian estimates. On real-world data sets, ADMIXTURE's estimates are directly comparable to those from structure and EIGENSTRAT. Taken together, our results show that ADMIXTURE's computational speed opens up the possibility of using a much larger set of markers in model-based ancestry estimation and that its estimates are suitable for use in correcting for population stratification in association studies. © 2009 by Cold Spring Harbor Laboratory Press.},
   author = {David H. Alexander and John Novembre and Kenneth Lange},
   doi = {10.1101/GR.094052.109},
   issn = {1088-9051},
   issue = {9},
   journal = {Genome Research},
   month = {9},
   pages = {1655-1664},
   pmid = {19648217},
   publisher = {Cold Spring Harbor Laboratory Press},
   title = {Fast model-based estimation of ancestry in unrelated individuals},
   volume = {19},
   url = {http://genome.cshlp.org/content/19/9/1655.full http://genome.cshlp.org/content/19/9/1655 http://genome.cshlp.org/content/19/9/1655.abstract},
   year = {2009}
}